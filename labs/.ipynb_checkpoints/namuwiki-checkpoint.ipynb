{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read namuwiki dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/namuwiki.json') as f:\n",
    "    context_ = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = defaultdict(dict)\n",
    "for doc in context_:\n",
    "    context[doc['title']][doc['namespace']] = doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_document = re.compile('\\[\\[(.[^:]+?)\\]\\]')\n",
    "regex_table = re.compile('(?<=\\|\\|)(.*)(?=\\|\\|)')\n",
    "regex_bracket = re.compile('\\((.+?)\\)')\n",
    "regex_redirect = re.compile('#redirect (.+?)')\n",
    "\n",
    "regex_tags = OrderedDict({\n",
    "    'horizontal_line': ('', re.compile('\\-{4,9}')),\n",
    "    'comment': ('', re.compile('##\\s?(.*)')),\n",
    "    \n",
    "    'header': (r'\\1', re.compile('\\={2,6}\\#?(.+?)\\#?\\={2,6}')),\n",
    "    'bold': (r'\\1', re.compile(\"\\'\\'\\'(.+?)\\'\\'\\'\")),\n",
    "    'italic': (r'\\1', re.compile(\"\\'\\'(.+?)\\'\\'\")),\n",
    "    'strike1': (r'\\1', re.compile('~~(.+?)~~')),\n",
    "    'strike2': (r'\\1', re.compile('--(.+?)--')),\n",
    "    'underline': (r'\\1', re.compile('__(.+?)__')),\n",
    "    'upper': (r'\\1', re.compile('\\^\\^(.+?)\\^\\^')),\n",
    "    'under': (r'\\1', re.compile(',,(.+?),,')),\n",
    "    \n",
    "    'bigger': (r'\\1', re.compile('\\{\\{\\{\\+[1-5] (.+?)\\}\\}\\}')),\n",
    "    'smaller': (r'\\1', re.compile('\\{\\{\\{\\-[1-5] (.+?)\\}\\}\\}')),\n",
    "    'color': (r'\\2', re.compile('\\{\\{\\{\\#(.+?) (.+?)\\}\\}\\}')),\n",
    "    'without_markup': (r'\\1', re.compile('\\{\\{\\{(.*)\\}\\}\\}')),\n",
    "    \n",
    "    'macro_html': (r'\\1', re.compile('\\{\\{\\{\\#\\!html (.+?)\\}\\}\\}')),\n",
    "    'macro_wiki': (r'\\2', re.compile('\\{\\{\\{\\#\\!wiki (.+?)\\n(.*)\\}\\}\\}')),\n",
    "    'macro_syntax': (r'\\2', re.compile('\\{\\{\\{\\#\\!syntax (.+?)\\n(.*)\\n\\}\\}\\}', re.IGNORECASE)),\n",
    "    'macro_color': (r'\\2', re.compile('\\{\\{\\{\\#(.+?) (.+?)\\}\\}\\}', re.IGNORECASE)),\n",
    "    'macro_math': ('', re.compile('\\[math\\((.+?)\\)\\]', re.IGNORECASE)),\n",
    "    'macro_date': ('', re.compile('\\[date(time)?\\]', re.IGNORECASE)),\n",
    "    'macro_br': ('\\n', re.compile('\\[br\\]', re.IGNORECASE)),\n",
    "    'macro_include': ('', re.compile('\\[include(.+?)\\]', re.IGNORECASE)),\n",
    "    'macro_index': ('', re.compile('\\[목차\\]')),\n",
    "    'macro_index_': ('', re.compile('\\[tableofcontents\\]')),\n",
    "    'macro_footnote': ('', re.compile('\\[각주\\]')),\n",
    "    'macro_footnote_': ('', re.compile('\\[footnote\\]')),\n",
    "    'macro_pagecount': ('', re.compile('\\[pagecount(.+?)?\\]', re.IGNORECASE)),\n",
    "    'macro_age': ('', re.compile('\\[age\\(\\)\\]', re.IGNORECASE)),\n",
    "    'macro_dday': ('', re.compile('\\[dday\\(\\)\\]', re.IGNORECASE)),\n",
    "    'macro_tag': ('', re.compile('\\<(.+?)\\>')),\n",
    "\n",
    "    'attach_': (r'\\1', re.compile('\\[\\[파일:(.+?)\\|(.+?)\\]\\]')),\n",
    "    'attach': (r'\\1', re.compile('\\[\\[파일:(.+?)\\]\\]')),\n",
    "    \n",
    "    'paragraph_': (r'\\1', re.compile('\\[\\[#s-(.+?)\\|(.+?)\\]\\]')),\n",
    "    'paragraph': (r'\\1', re.compile('\\[\\[#s-(.+?)\\]]')),\n",
    "    'link_paragraph_': (r'\\1', re.compile('\\[\\[(.+?)#s-(.+?)\\|(.+?)\\]\\]')),\n",
    "    'link_paragraph': (r'\\1', re.compile('\\[\\[(.+?)#s-(.+?)\\]\\]')),\n",
    "    'link': (r'\\1', re.compile('\\[\\[((?:(?!\\|).)+?)\\]\\]')),\n",
    "    'link_': (r'\\1', re.compile('\\[\\[(.+?)\\|(.+?)\\]\\]')),\n",
    "    \n",
    "    'list': (r'\\1', re.compile('\\|\\*\\|(.*)')),\n",
    "    'list_': (r'\\1', re.compile('\\|\\*(.*)')),\n",
    "    'list__': (r'\\1', re.compile('\\|[1Aa]\\.\\|(.*)')),\n",
    "    'list___': (r'\\1', re.compile('\\|[1Aa]\\.(.*)')),\n",
    "    'unordred_list': (r'\\1', re.compile('[ ]+\\*(.*)')),\n",
    "    'ordered_list': (r'\\1', re.compile('[ ]+[1AaIi]\\.(.*)')),\n",
    "    'quote': (r'\\1', re.compile('\\>+\\s?(.*)')),\n",
    "\n",
    "    'footnote': ('', re.compile('\\[\\*[A-Za-z]? (.+?)\\]')),    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(text: str, verbose: bool = False) -> str:\n",
    "    def _parse(text: str, target: str, tag: re.Pattern) \\\n",
    "        -> Tuple[str, int]:\n",
    "        return tag.subn(target, text)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Parsing Regex {len(regex_tags.keys())} rules\\n\\t{text}')\n",
    "    \n",
    "    while True:\n",
    "        count = 0\n",
    "        for key, (target, tag) in regex_tags.items():\n",
    "            text, count = _parse(text, target, tag)\n",
    "            if count:\n",
    "                if verbose:\n",
    "                    print(f'Rule [{key}: {tag}]\\n\\t{text}')\n",
    "                break\n",
    "        if not count:\n",
    "            break\n",
    "            \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_table(text):\n",
    "    for row in regex_table.findall(text):\n",
    "        values = row.split('||')\n",
    "        yield values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get drama titles from channles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "interested = [\"JTBC 금토 드라마(2014~2017)\", \"JTBC 금토 드라마(2017~2020)\", \"JTBC 드라마\", \"JTBC 수목 드라마\", \"JTBC 월화 드라마(2011~2014)\", \"JTBC 월화 드라마(2017~2020)\", \"JTBC 주말 드라마\", \"KBS 수목 드라마(2001~2005)\", \"KBS 수목 드라마(2006~2010)\", \"KBS 수목 드라마(2011~2015)\", \"KBS 수목 드라마(2016~2020)\", \"KBS 월화 드라마(2001~2005)\", \"KBS 월화 드라마(2006~2010)\", \"KBS 월화 드라마(2011~2015)\", \"KBS 월화 드라마(2016~2020)\", \"KBS 학교 시리즈\", \"MBC 수목 미니시리즈(2006~2010)\", \"MBC 수목 미니시리즈(2011~2015)\", \"MBC 수목 미니시리즈(2016~2020)\", \"MBC 아침 드라마(2011~2015)\", \"MBC 아침 드라마(2016~2020)\", \"MBC 예능 드라마\", \"MBC 월화 미니시리즈(2006~2010)\", \"MBC 월화 미니시리즈(2016~2020)\", \"MBC 월화특별기획(2011~2015)\", \"MBC 일일 드라마(2016~2020)\", \"MBC 일일 연속극(2011~2015)\", \"MBC 주말 드라마(2011~2015)\", \"MBC 주말 드라마(2016~2020)\", \"MBC 주말 특별기획(2011~2015)\", \"MBC 주말 특별기획(2016~2020)\", \"MBC 하이킥 시리즈\", \"MBN 수목 드라마\", \"OCN 로맨스 드라마\", \"OCN 수목 오리지널\", \"OCN 오리지널 드라마(2010~2016)\", \"OCN 월화 오리지널\", \"OCN 토일 오리지널(2017~2020)\", \"SBS 금토 드라마(2019~현재)\", \"SBS 드라마 스페셜(1992~1995)\", \"SBS 드라마 스페셜(1996~2000)\", \"SBS 드라마 스페셜(2001~2005)\", \"SBS 드라마 스페셜(2006~2010)\", \"SBS 드라마 스페셜(2011~2015)\", \"SBS 드라마 스페셜(2016~2020)\", \"SBS 아침 연속극(2016~2020)\", \"SBS 월화 드라마(1991~1995)\", \"SBS 월화 드라마(1996~2000)\", \"SBS 월화 드라마(2001~2005)\", \"SBS 월화 드라마(2006~2010)\", \"SBS 월화 드라마(2011~2015)\", \"SBS 월화 드라마(2016~2020)\", \"TV CHOSUN 토일드라마\", \"tvN 금요 드라마(2007~2015)\", \"tvN 금토 드라마\", \"tvN 로맨스가 필요해 시리즈\", \"tvN 불금 시리즈(2017~)\", \"tvN 월화 드라마(2011~2015)\", \"tvN 월화 드라마(2016~2020)\", \"tvN 토일 드라마(2017~2020)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inter in interested:\n",
    "    document = context[inter]\n",
    "    matches = regex_document.findall(document['1']['text'])\n",
    "    \n",
    "    for match in matches:\n",
    "        name, *_ = match.split('|')\n",
    "        name, *_ = name.split('#')\n",
    "        titles.add(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get metadata from document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = {\n",
    "    '방송 기간': ['방송기간', '방송 기간'],\n",
    "    '방송 횟수': ['횟수', '방송 횟수'],\n",
    "    '장르': ['장르'],\n",
    "    '채널': ['채널', '방송사'],\n",
    "    '제작사': ['제작사', '제작자', '제작'],\n",
    "    '극본': ['극본', '대본'],\n",
    "    '출연자': ['출연자', '출연', '출연진'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = defaultdict(dict)\n",
    "notexists = []\n",
    "notparser = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in titles:\n",
    "    try:\n",
    "        if '(드라마)' not in title and f'{title}(드라마)' in context:\n",
    "            document = context[title]['0']['text']\n",
    "        else:\n",
    "            document = context[title]['0']['text']\n",
    "    except KeyError:\n",
    "        notexists.append(title)\n",
    "    \n",
    "    d = defaultdict(str)\n",
    "    for row in parse_table(document):\n",
    "        try:\n",
    "            key, value = filter(len, map(parse, row))\n",
    "        except ValueError:\n",
    "            continue\n",
    "        key = next((ckey for ckey, cvalues in columns.items() if any(cvalue in key for cvalue in cvalues)), False)\n",
    "        if key:\n",
    "            key = key.strip()\n",
    "            d[key] = f'{d[key]} {value}'\n",
    "    \n",
    "    if not d:\n",
    "        notparser.append(title)\n",
    "    else:\n",
    "        data[title] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 pages are not parsable\n",
      "2 pages are not exists\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(notparser)} pages are not parsable')\n",
    "print(f'{len(notexists)} pages are not exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create table from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_columns = list(columns.keys())\n",
    "table = np.empty((0, len(table_columns) + 1))\n",
    "\n",
    "for title, values in data.items():\n",
    "    table = np.vstack((table, np.array([\n",
    "        regex_bracket.sub('', title).replace(' ', '').strip(), *tuple(map(lambda c: values[c], table_columns))\n",
    "    ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(table)\n",
    "df.columns = ['제목', *table_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_date = re.compile('(.+?)년(.*)월(.*)일')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start = []\n",
    "for index, date in enumerate(df['방송 기간']):\n",
    "    ds, *de = map(regex_date.findall, map(str.strip, date.split('~' if '~' in date else '-')))\n",
    "    ds, *_ = ds or ['unknown']\n",
    "    try:\n",
    "        date_start.append(pd.datetime(*tuple(map(int, ds))))\n",
    "    except (ValueError, TypeError):\n",
    "        date_start.append('unknown')\n",
    "    \n",
    "assert len(date_start) == np.size(df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['방송 시작'] = pd.Series(date_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(735, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>제목</th>\n",
       "      <th>방송 기간</th>\n",
       "      <th>방송 횟수</th>\n",
       "      <th>장르</th>\n",
       "      <th>채널</th>\n",
       "      <th>제작사</th>\n",
       "      <th>극본</th>\n",
       "      <th>출연자</th>\n",
       "      <th>방송 시작</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>외과의사봉달희</td>\n",
       "      <td>2007년 1월 17일 ~ 2007년 3월 15일</td>\n",
       "      <td>18부작</td>\n",
       "      <td>의학 드라마</td>\n",
       "      <td>SBS</td>\n",
       "      <td>DSP미디어</td>\n",
       "      <td>이정선</td>\n",
       "      <td>이요원, 이범수, 김민준, 오윤아 外</td>\n",
       "      <td>2007-01-17 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>단,하나의사랑</td>\n",
       "      <td>2019년 5월 22일 ~ 2019년  7월 11일</td>\n",
       "      <td>32부작</td>\n",
       "      <td>판타지, 로맨스</td>\n",
       "      <td>KBS 2TV</td>\n",
       "      <td>빅토리콘텐츠, 몬스터유니온</td>\n",
       "      <td></td>\n",
       "      <td>신혜선, 엘(인피니트), 이동건, 김보미(1987) 外</td>\n",
       "      <td>2019-05-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>백일의낭군님</td>\n",
       "      <td>2018년 9월 10일 ~ 10월 30일</td>\n",
       "      <td>16부작</td>\n",
       "      <td>미스터리 로맨스 픽션사극</td>\n",
       "      <td>tvN</td>\n",
       "      <td>에이스토리</td>\n",
       "      <td>노지설</td>\n",
       "      <td>도경수, 남지현 外</td>\n",
       "      <td>2018-09-10 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>러브홀릭</td>\n",
       "      <td>2005년 5월 2일 ~ 2005년 6월 21일</td>\n",
       "      <td>16부작</td>\n",
       "      <td>멜로, 드라마</td>\n",
       "      <td>KBS 2TV</td>\n",
       "      <td>KBS 2TV 자체제작</td>\n",
       "      <td>이향희</td>\n",
       "      <td>김규리(1979년 10월), 강타, 이선균, 유인영 外</td>\n",
       "      <td>2005-05-02 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>왜그래풍상씨</td>\n",
       "      <td>2019년 1월 9일 ~ 2019년 3월 14일</td>\n",
       "      <td>40부작 + 스페셜 1부작</td>\n",
       "      <td></td>\n",
       "      <td>KBS 2TV</td>\n",
       "      <td>초록뱀미디어, 팬 엔터테인먼트</td>\n",
       "      <td>문영남</td>\n",
       "      <td>유준상, 오지호(배우), 전혜빈, 이시영(배우), 이창엽(배우) 外</td>\n",
       "      <td>2019-01-09 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        제목                         방송 기간           방송 횟수             장르  \\\n",
       "0  외과의사봉달희   2007년 1월 17일 ~ 2007년 3월 15일            18부작         의학 드라마   \n",
       "1  단,하나의사랑  2019년 5월 22일 ~ 2019년  7월 11일            32부작       판타지, 로맨스   \n",
       "2   백일의낭군님        2018년 9월 10일 ~ 10월 30일            16부작  미스터리 로맨스 픽션사극   \n",
       "3     러브홀릭    2005년 5월 2일 ~ 2005년 6월 21일            16부작        멜로, 드라마   \n",
       "4   왜그래풍상씨    2019년 1월 9일 ~ 2019년 3월 14일  40부작 + 스페셜 1부작                  \n",
       "\n",
       "        채널               제작사   극본                                    출연자  \\\n",
       "0      SBS            DSP미디어  이정선                   이요원, 이범수, 김민준, 오윤아 外   \n",
       "1  KBS 2TV    빅토리콘텐츠, 몬스터유니온              신혜선, 엘(인피니트), 이동건, 김보미(1987) 外   \n",
       "2      tvN             에이스토리  노지설                             도경수, 남지현 外   \n",
       "3  KBS 2TV      KBS 2TV 자체제작  이향희         김규리(1979년 10월), 강타, 이선균, 유인영 外   \n",
       "4  KBS 2TV  초록뱀미디어, 팬 엔터테인먼트  문영남  유준상, 오지호(배우), 전혜빈, 이시영(배우), 이창엽(배우) 外   \n",
       "\n",
       "                 방송 시작  \n",
       "0  2007-01-17 00:00:00  \n",
       "1  2019-05-22 00:00:00  \n",
       "2  2018-09-10 00:00:00  \n",
       "3  2005-05-02 00:00:00  \n",
       "4  2019-01-09 00:00:00  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../results/namuwiki.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
